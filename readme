git remote add upstream https://github.com/linux-nvme/nvme-cli.git
git fetch upstream
git checkout -b v1.8.1


所有命令: nvme-builtin.h, 如: ENTRY("write" -> write_cmd
submit_io(nvme_cmd_write, "write", desc, argc, argv)
  parse_and_open
  nvme_io
    ioctl(fd, NVME_IOCTL_SUBMIT_IO, &io) -> 转到内核处理


client
nvme discover -t rdma -a 192.168.80.100 -s 4420 #-s service id, port
nvme connect -t rdma -n "nqn.2016-06.io.spdk:cnode1" -a 192.168.80.100 -s 4420

nvme discover -t rdma -a 172.17.29.65 -s 4421

gdb --args nvme discover -t rdma -a 172.17.29.217 -s 4420 -> admin_passthru
gdb --args /usr/sbin/nvme discover -t rdma -s 4420 -a 172.17.29.217
nvme.c -> main -> int main(int argc, char **argv)
handle_plugin -> int handle_plugin
plugin->commands[i] -> COMMAND_LIST -> ENTRY("discover", "Discover NVMeoF subsystems", discover_cmd)
cmd->fn(argc, argv, cmd, plugin) -> static int discover_cmd
  discover(desc, argc, argv, false) -> int discover(const char *desc, int argc, char **argv, bool connect)
    argconfig_parse
    build_options -> static int build_options
    static int do_discover(char *argstr, bool connect) -> nqn=nqn.2014-08.org.nvmexpress.discovery,transport=rdma,traddr=172.17.29.217,trsvcid=4420,hostnqn=nqn.2014-08.org.nvmexpress:uuid:e4a72828-9b7f-454f-ab8a-60b2c57e2439,hostid=cee6f16a-0183-4d00-ac0f-58
      add_ctrl -> static int add_ctrl(const char *argstr) -> 控制器设备: /dev/nvme0
        open(PATH_NVME_FABRICS, O_RDWR)
        write(fd, argstr, len) -> 写设备, 触发内核驱动处理(nvmf_dev_write) -> nqn=nqn.2014-08.org.nvmexpress.discovery,transport=rdma,traddr=175.17.53.73,trsvcid=4420,hostnqn=nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7,hostid=a20d3ab6-2c0a-4335-8552-305 -> 
          Jul 12 11:13:56 s63 kernel: nvme nvme0: new ctrl: NQN "nqn.2014-08.org.nvmexpress.discovery", addr 172.17.29.65:4420
          Jul 12 11:14:01 s63 systemd: Started Session 3337 of user root.
        read(fd, buf, BUF_SIZE)
      nvmf_get_log_page_discovery -> static int nvmf_get_log_page_discovery
        nvme_discovery_log
          nvme_get_log
            nvme_get_log13
      remove_ctrl
      nvmf_get_log_page_discovery -> static int nvmf_get_log_page_discovery -> /dev/nvme0
        nvme_discovery_log -> int nvme_get_log -> nvme_get_log13 -> .opcode		= nvme_admin_get_log_page -> return ioctl(fd, ioctl_cmd, cmd) -> 管理命令: nvme_admin_get_log_page		= 0x02
        enum nvme_admin_opcode nvme管理命令 -> linux/nvme.h
        nvme_submit_admin_passthru
          ioctl(fd, NVME_IOCTL_ADMIN_CMD, cmd) -> 转到内核驱动处理 -> nvme_dev_ioctl
      remove_ctrl
      case DISC_OK
      ret = connect_ctrls(log, numrec)
      case DISC_NO_LOG
      print_discovery_log
    connect_ctrls



#define PATH_NVME_FABRICS	"/dev/nvme-fabrics"

 
gdb --args nvme nvme connect -t rdma -n nvme-subsystem-name -a 172.17.29.65 -s 4421
gdb --args nvme connect -t rdma -n "nqn.2022-06.io.spdk:cnode216" -a 172.17.29.217 -s 4420
main -> handle_plugin

... -> ENTRY("connect", "Connect to NVMeoF subsystem", connect_cmd)
connect_cmd -> int connect
  ret = argconfig_parse(argc, argv, desc, command_line_options, &cfg
  build_options
  instance = add_ctrl(argstr) -> static int add_ctrl -> 添加控制器
    fd = open(PATH_NVME_FABRICS, O_RDWR) -> PATH_NVME_FABRICS	"/dev/nvme-fabrics"
    if (write(fd, argstr, len) -> nqn=nvme-subsystem-name,transport=rdma,traddr=172.17.29.65,trsvcid=4421,hostnqn=nqn.2014-08.org.nvmexpress:uuid:3195faad-fe20-44d5-8ae4-291d29629c89,hostid=a872f1d1-daae-4da0-be86-5a36131e640b -> [root@s63 ~]# bpftrace -e 'kprobe:nvmf_dev_write{ printf("%s\n", kstack); }' -> nvmf_dev_write
    len = read(fd, buf, BUF_SIZE)
    case OPT_INSTANCE




# 与target 端 nvme ssd 断开连接
nvme disconnect -n nqn.2022-06.io.spdk:cnode216


[root@node215 nvme-cli-1.8.1]# nvme list -v
Node             SN                   Model                                    Namespace Usage                      Format           FW Rev  
---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
/dev/nvme0n1     SPDK00000000000001   SPDK_Controll                            1         536.87  MB / 536.87  MB      2 KiB +  0 B   21.10


ls /dev/nvme*
tree -lL 1 /sys/class/misc/nvme-fabrics/




static void print_discovery_log
Discovery Log Number of Records 1, Generation counter 1
=====Discovery Log Entry 0======
trtype:  rdma
adrfam:  ipv4
subtype: nvme subsystem
treq:    not required
portid:  0
trsvcid: 4420
subnqn:  nqn.2022-06.io.spdk:cnode216
traddr:  172.17.29.63
rdma_prtype: not specified
rdma_qptype: connected
rdma_cms:    rdma-cm
rdma_pkey: 0x0000


dd if=/dev/urandom of=4k bs=4k count=1
rm -rf 4k; for i in {0..4095};do printf a >> 4k;done; stat 4k
gdb --args ./nvme write /dev/nvme1n1 --start-block=0 --block-count=1 --data-size=4k --data=./4k -v
$8 = {
  start_block = 0, 
  block_count = 1, 
  data_size = 4096, 
  metadata_size = 0, 
  ref_tag = 0, 
  data = 0x434dd2 "", 
  metadata = 0x434dd2 "", 
  prinfo = 0 '\000', 
  dtype = 0 '\000', 
  dspec = 0, 
  dsmgmt = 0, 
  app_tag_mask = 0, 
  app_tag = 0, 
  limited_retry = 0, 
  force_unit_access = 0, 
  show = 0, 
  dry_run = 0, 
  latency = 0
}
write_cmd -> submit_io
  fd = parse_and_open -> 打开设备
  buffer_size = (cfg.block_count + 1) * phys_sector_size -> 2 * 512 = 1024, 修复读/写内存损坏问题，当向设备发出读操作时，如果我们传递的数据大小小于当前扇区大小，ioctl() 将溢出数据缓冲区。 如果缓冲区没有足够的字节来存储结果，从设备读取多个块时也会发生这种情况。 此补丁使用请求的块计数添加了一个简单的检查，以正确分配最小大小的缓冲区
  buffer_size = cfg.data_size = 4096
  posix_memalign(&buffer, getpagesize(), buffer_size) -> 分配对齐的内存
  memset(buffer, 0, buffer_size)
  err = read(dfd, (void *)buffer, cfg.data_size)
  ioctl(fd, BLKPBSZGET, &phys_sector_size)
  nvme_io
    ioctl(fd, NVME_IOCTL_SUBMIT_IO, &io)
(gdb) p io
$2 = {
  opcode = 1 '\001', 
  flags = 0 '\000', 
  control = 0, 
  nblocks = 1, 
  rsvd = 0, 
  metadata = 0, 
  addr = 6635520, 
  slba = 0, 
  dsmgmt = 0, 
  reftag = 0, 
  apptag = 0, 
  appmask = 0
}




